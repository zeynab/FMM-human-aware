\section{Experiments}
\label{sec:experimental_setup}
In this section we will briefly explain our robotic platform, experimental setup and the set of experiments along with the metrics which have been conducted to test our uncertainty-based human-aware navigation system.

\subsection{Robotic Platform}
\label{sec:robot}

\begin{figure}[t!]
\centering
\subfloat[]{\includegraphics[width=0.12\textwidth]{pictures/robot.jpg}\label{fig:robot}}%
\hspace{0.1cm}
\subfloat[]{\includegraphics[width=0.32\textwidth]{pictures/arena.png}\label{fig:arena}}%
\hspace{0.1cm}

\caption{(a) Mbot robotic platform. (b) Experimental arena. The robot starts from its initial position and navigates to the end point while dealing with the people in the environment. This is a snapshot of the first step of the single static person case.}
\label{fig:setup}
\end{figure}


%\begin{figure}
%    \centering
%    \includegraphics[scale=0.07]{pictures/robot.jpg}
%    \caption{Robot used in our experiments to perform the automatic fingerprinting.}
%     \label{robot}
%\end{figure}


The robotic platform used in this work is shown in Figure \ref{fig:robot}. This robot is called mBot \cite{Messias2014robotic} and is developed within the FP7 European project MOnarCH.
It is an omni-directional drive robot with an approximately round footprint of 0.65 m in diameter and a height of 0.98 m.
It is provided with two laser range finders, on both the front and the back for providing full coverage.

Two batteries give it an autonomy of approximately five hours, depending on the usage.
The robot has two PCs inside its shell: one manages the sensors, navigation and actuators, while the second one is for other functions such as human-robot interaction functionalities, which are outside our interests for this work. The two on-board PCs, run Ubuntu desktop 12.04 and ROS Hydro. 


\subsection{Experimental Setup}
\label{sec:Experimental_setup}


For evaluating the performance of our methods we have used an extensive suit of experiments both in simulation and reality. A high fidelity robotic simulator Webots \cite{michel1998webots}, with realistic models of the mBot and the testing environment has been used for testing in the initial step. The trackers have been emulated initially but real recorded data bags of perception, were used in the next steps for more close to reality simulations. This was done to simplify the debugging process and speeding up the robotic experiments. %by replaying real uncertainty and tracking data from real experiments.
Finally, we tested our method in reality in three different scenarios, in a laboratory environment of 5 m $\times$ 7 m, depicted in Figure \ref{fig:arena} where the tracker is operational.
 
%\begin{figure*}[t]
%\centering
%\includegraphics[width=0.30\textwidth]{pictures/arena.png}
%\caption{Connection of different system components.} %The social navigation layer adds the costs associated to people to other costmaps for taking path planning decisions.}
%\label{fig:arena}
%\end{figure*}


People detection and tracking is done by means of an omni-directional overhead camera \zt{Deepak please add the camera information here, model and the rest of the related stuff} with \zt{?} frames per second. The deterministic tracker outputs results with  \zt{?} hz and the probabilistic tracker with  \zt{?} hz.  \zt{maybe a table for everything here}. The ground truth position of the robot is given by AMCL with 5-10 cm accuracy, and the person stands and walks on physically marked tracks to get the exact precise ground truth.


\subsection{Scenarios}
\label{sec:scenarios}

We have studied three different scenarios for each of our human-aware navigation methods, each having been tested five times. We start with a single static person and incrementally increase the complexity to one moving person and finally, two static people in the arena. We should emphasize that perception uncertainty is affecting the tracking performance and is not evident or quantifiable from just looking at the environment. This means, the person is not aware of what is happening on the tracker side, however, the information given by the tracker greatly affects the behavior of the robot, and therefore the social acceptability. 


Since we aim to study the effect of perception uncertainty in human-aware navigation we chose a task of point to point navigation for the robot in the vicinity of humans, where the navigation task itself is very basic. The complexity of navigation task can be increased and more interesting scenarios in terms of HRI can be investigated but that is outside the scope of our work. 

In each experiment, the robot starts from a predefined starting point and is sent one predefined goal. The robot then has to behave appropriately when encountered with people in the arena. For the static case there is always a person standing between the robot and the straight line to the goal, and for the dynamic case the person moves along this line as the robot starts navigating towards the goal causing a direct encounter with the robot. The following section will explain the metrics used for performance assessment in our experiments.

%
%Figure shows a snapshot of the setup with two people moving in the arena. For the case of static people we have also done extensive tests using a high fidelity simulator webots \cite{michel1998webots}, by 1) emulating the uncertainty of tracking, and 2) replaying real uncertainty and tracking data from real experiments. 




\subsection{Metrics}
A widely accepted description of the speed profile in neurobiology is based on the ‘minimal jerk criterion’ \cite{burdet1998quantization}.



remarks:
convolution : It is easier to discard small probabilities that are random measurements introduced by the probabilistic MCMC-based tracker, using this representation comparing to clustering methods.
